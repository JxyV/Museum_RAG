"""
è¯­éŸ³æ¥å£æ¨¡å— - æ”¯æŒå¤šç§STTå’ŒTTSæ¨¡å‹
"""
import os
import io
import time
import logging
from typing import Optional, Dict, Any
from abc import ABC, abstractmethod

import pyaudio
import wave
import numpy as np
from dotenv import load_dotenv

# STT æ¨¡å‹å¯¼å…¥
# try:
#     import whisper
#     WHISPER_AVAILABLE = True
# except ImportError:
#     WHISPER_AVAILABLE = False

# try:
#     from speech_recognition import AudioData, Recognizer, Microphone
#     SPEECH_RECOGNITION_AVAILABLE = True
# except ImportError:
#     SPEECH_RECOGNITION_AVAILABLE = False

# TTS æ¨¡å‹å¯¼å…¥
# try:
#     import edge_tts
#     EDGE_TTS_AVAILABLE = True
# except ImportError:
#     EDGE_TTS_AVAILABLE = False

# try:
#     from TTS.api import TTS
#     COQUI_TTS_AVAILABLE = True
# except ImportError:
#     COQUI_TTS_AVAILABLE = False

# try:
#     import pyttsx3
#     PYTTSX3_AVAILABLE = True
# except ImportError:
#     PYTTSX3_AVAILABLE = False


class STTModel(ABC):
    """è¯­éŸ³è½¬æ–‡æœ¬åŸºç±»"""
    
    @abstractmethod
    def transcribe(self, audio_data: bytes) -> str:
        pass


class WhisperSTT(STTModel):
    """Whisper STTæ¨¡å‹"""
    
    def __init__(self, model_size: str = "base"):
        if not WHISPER_AVAILABLE:
            raise ImportError("whisper not installed. Run: pip install openai-whisper")
        
        self.model = whisper.load_model(model_size)
        logging.info(f"Whisper model {model_size} loaded")
    
    def transcribe(self, audio_data: bytes) -> str:
        # å°†éŸ³é¢‘æ•°æ®ä¿å­˜ä¸ºä¸´æ—¶æ–‡ä»¶
        temp_file = "temp_audio.wav"
        with open(temp_file, "wb") as f:
            f.write(audio_data)
        
        try:
            result = self.model.transcribe(temp_file, language="zh")
            return result["text"].strip()
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)


class SpeechRecognitionSTT(STTModel):
    """ä½¿ç”¨speech_recognitionåº“çš„STT"""
    
    def __init__(self, engine: str = "google"):
        if not SPEECH_RECOGNITION_AVAILABLE:
            raise ImportError("speech_recognition not installed. Run: pip install SpeechRecognition")
        
        self.recognizer = Recognizer()
        self.engine = engine
        logging.info(f"SpeechRecognition STT initialized with {engine}")
    
    def transcribe(self, audio_data: bytes) -> str:
        # å°†bytesè½¬æ¢ä¸ºAudioDataå¯¹è±¡
        audio = AudioData(audio_data, 16000, 2)  # å‡è®¾16kHz, 16bit
        
        try:
            if self.engine == "google":
                text = self.recognizer.recognize_google(audio, language="zh-CN")
            elif self.engine == "sphinx":
                text = self.recognizer.recognize_sphinx(audio)
            else:
                text = self.recognizer.recognize_google(audio, language="zh-CN")
            
            return text.strip()
        except Exception as e:
            logging.error(f"STT recognition failed: {e}")
            return ""


class TTSModel(ABC):
    """æ–‡æœ¬è½¬è¯­éŸ³åŸºç±»"""
    
    @abstractmethod
    def synthesize(self, text: str) -> bytes:
        pass


class EdgeTTS(TTSModel):
    """Edge TTSæ¨¡å‹"""
    
    def __init__(self, voice: str = "zh-CN-XiaoxiaoNeural"):
        if not EDGE_TTS_AVAILABLE:
            raise ImportError("edge-tts not installed. Run: pip install edge-tts")
        
        self.voice = voice
        logging.info(f"Edge TTS initialized with voice: {voice}")
    
    def synthesize(self, text: str) -> bytes:
        import asyncio
        
        async def _synthesize():
            communicate = edge_tts.Communicate(text, self.voice)
            return await communicate.as_bytes()
        
        return asyncio.run(_synthesize())


class CoquiTTS(TTSModel):
    """Coqui TTSæ¨¡å‹"""
    
    def __init__(self, model_name: str = "tts_models/zh-CN/baker/tacotron2-DDC-GST"):
        if not COQUI_TTS_AVAILABLE:
            raise ImportError("TTS not installed. Run: pip install TTS")
        
        self.tts = TTS(model_name)
        logging.info(f"Coqui TTS initialized with model: {model_name}")
    
    def synthesize(self, text: str) -> bytes:
        # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶
        temp_file = "temp_output.wav"
        self.tts.tts_to_file(text=text, file_path=temp_file)
        
        try:
            with open(temp_file, "rb") as f:
                audio_data = f.read()
            return audio_data
        finally:
            if os.path.exists(temp_file):
                os.remove(temp_file)


class Pyttsx3TTS(TTSModel):
    """Pyttsx3 TTSæ¨¡å‹ï¼ˆç³»ç»Ÿå†…ç½®ï¼‰"""
    
    def __init__(self, rate: int = 200, volume: float = 0.9):
        if not PYTTSX3_AVAILABLE:
            raise ImportError("pyttsx3 not installed. Run: pip install pyttsx3")
        
        self.engine = pyttsx3.init()
        self.engine.setProperty('rate', rate)
        self.engine.setProperty('volume', volume)
        
        # è®¾ç½®ä¸­æ–‡è¯­éŸ³
        voices = self.engine.getProperty('voices')
        for voice in voices:
            if 'chinese' in voice.name.lower() or 'zh' in voice.id.lower():
                self.engine.setProperty('voice', voice.id)
                break
        
        logging.info("Pyttsx3 TTS initialized")
    
    def synthesize(self, text: str) -> bytes:
        # Pyttsx3 ä¸ç›´æ¥è¿”å›éŸ³é¢‘æ•°æ®ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦ä¿å­˜åˆ°æ–‡ä»¶
        temp_file = "temp_pyttsx3.wav"
        self.engine.save_to_file(text, temp_file)
        self.engine.runAndWait()
        
        try:
            with open(temp_file, "rb") as f:
                audio_data = f.read()
            return audio_data
        finally:
            if os.path.exists(temp_file):
                os.remove(temp_file)


class VoiceInterface:
    """è¯­éŸ³æ¥å£ä¸»ç±»"""
    
    def __init__(self):
        load_dotenv()
        self.setup_logging()
        
        # åˆå§‹åŒ–STTå’ŒTTS
        self.stt = self._create_stt()
        self.tts = self._create_tts()
        
        # éŸ³é¢‘å‚æ•°
        self.chunk = 1024
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000
        self.record_seconds = 5  # é»˜è®¤å½•éŸ³5ç§’
        
        self.audio = pyaudio.PyAudio()
    
    def setup_logging(self):
        level = os.getenv("LOG_LEVEL", "INFO").upper()
        logging.basicConfig(level=level, format="%(asctime)s | %(levelname)s | %(message)s")
    
    def _create_stt(self) -> STTModel:
        stt_backend = os.getenv("STT_BACKEND", "whisper").lower()
        stt_model = os.getenv("STT_MODEL", "base")
        
        if stt_backend == "whisper":
            return WhisperSTT(model_size=stt_model)
        elif stt_backend == "speech_recognition":
            engine = os.getenv("STT_ENGINE", "google")
            return SpeechRecognitionSTT(engine=engine)
        else:
            raise ValueError(f"Unsupported STT backend: {stt_backend}")
    
    def _create_tts(self) -> TTSModel:
        tts_backend = os.getenv("TTS_BACKEND", "edge").lower()
        
        if tts_backend == "edge":
            voice = os.getenv("TTS_VOICE", "zh-CN-XiaoxiaoNeural")
            return EdgeTTS(voice=voice)
        elif tts_backend == "coqui":
            model = os.getenv("TTS_MODEL", "tts_models/zh-CN/baker/tacotron2-DDC-GST")
            return CoquiTTS(model_name=model)
        elif tts_backend == "pyttsx3":
            rate = int(os.getenv("TTS_RATE", "200"))
            volume = float(os.getenv("TTS_VOLUME", "0.9"))
            return Pyttsx3TTS(rate=rate, volume=volume)
        else:
            raise ValueError(f"Unsupported TTS backend: {tts_backend}")
    
    def record_audio(self, duration: Optional[int] = None) -> bytes:
        """å½•åˆ¶éŸ³é¢‘"""
        duration = duration or self.record_seconds
        
        print(f"ğŸ¤ å¼€å§‹å½•éŸ³ ({duration}ç§’)...")
        
        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )
        
        frames = []
        for _ in range(0, int(self.rate / self.chunk * duration)):
            data = stream.read(self.chunk)
            frames.append(data)
        
        stream.stop_stream()
        stream.close()
        
        # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºWAVæ ¼å¼
        wav_buffer = io.BytesIO()
        with wave.open(wav_buffer, 'wb') as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(self.audio.get_sample_size(self.format))
            wf.setframerate(self.rate)
            wf.writeframes(b''.join(frames))
        
        print("âœ… å½•éŸ³å®Œæˆ")
        return wav_buffer.getvalue()
    
    def transcribe_audio(self, audio_data: bytes) -> str:
        """å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬"""
        print("ğŸ”„ æ­£åœ¨è¯†åˆ«è¯­éŸ³...")
        start_time = time.perf_counter()
        
        text = self.stt.transcribe(audio_data)
        
        end_time = time.perf_counter()
        print(f"âœ… è¯†åˆ«å®Œæˆ ({end_time - start_time:.1f}ç§’): {text}")
        
        return text
    
    def synthesize_speech(self, text: str) -> bytes:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³"""
        print("ğŸ”„ æ­£åœ¨åˆæˆè¯­éŸ³...")
        start_time = time.perf_counter()
        
        audio_data = self.tts.synthesize(text)
        
        end_time = time.perf_counter()
        print(f"âœ… åˆæˆå®Œæˆ ({end_time - start_time:.1f}ç§’)")
        
        return audio_data
    
    def play_audio(self, audio_data: bytes):
        """æ’­æ”¾éŸ³é¢‘"""
        print("ğŸ”Š æ­£åœ¨æ’­æ”¾...")
        
        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            output=True
        )
        
        # ä»WAVæ•°æ®ä¸­æå–éŸ³é¢‘å¸§
        wav_buffer = io.BytesIO(audio_data)
        with wave.open(wav_buffer, 'rb') as wf:
            data = wf.readframes(1024)
            while data:
                stream.write(data)
                data = wf.readframes(1024)
        
        stream.stop_stream()
        stream.close()
        print("âœ… æ’­æ”¾å®Œæˆ")
    
    def voice_to_text(self, duration: Optional[int] = None) -> str:
        """å®Œæ•´çš„è¯­éŸ³è½¬æ–‡æœ¬æµç¨‹"""
        audio_data = self.record_audio(duration)
        return self.transcribe_audio(audio_data)
    
    def text_to_voice(self, text: str):
        """å®Œæ•´çš„æ–‡æœ¬è½¬è¯­éŸ³æµç¨‹"""
        audio_data = self.synthesize_speech(text)
        self.play_audio(audio_data)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.audio.terminate()


def test_voice_interface():
    """æµ‹è¯•è¯­éŸ³æ¥å£"""
    try:
        voice = VoiceInterface()
        
        print("=== è¯­éŸ³æ¥å£æµ‹è¯• ===")
        print("1. æµ‹è¯•è¯­éŸ³è½¬æ–‡æœ¬...")
        text = voice.voice_to_text(duration=3)
        print(f"è¯†åˆ«ç»“æœ: {text}")
        
        if text.strip():
            print("\n2. æµ‹è¯•æ–‡æœ¬è½¬è¯­éŸ³...")
            voice.text_to_voice(text)
        
        voice.cleanup()
        print("\nâœ… æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        logging.exception("Voice interface test failed")


if __name__ == "__main__":
    test_voice_interface()
